{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:11:54.646847Z",
     "iopub.status.busy": "2025-02-06T15:11:54.646487Z",
     "iopub.status.idle": "2025-02-06T15:16:40.146725Z",
     "shell.execute_reply": "2025-02-06T15:16:40.145071Z",
     "shell.execute_reply.started": "2025-02-06T15:11:54.646814Z"
    },
    "id": "2eSvM9zX_2d3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install \"torch==2.4.0\" \"xformers==0.0.27.post2\" triton torchvision torchaudio\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install tensorboard\n",
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:16:40.148902Z",
     "iopub.status.busy": "2025-02-06T15:16:40.148442Z",
     "iopub.status.idle": "2025-02-06T15:17:18.646513Z",
     "shell.execute_reply": "2025-02-06T15:17:18.645640Z",
     "shell.execute_reply.started": "2025-02-06T15:16:40.148856Z"
    },
    "id": "QmUBVEnvCDJv",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching Xformers to fix some performance issues.\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.4: Fast Llama patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e38869824249a29fe6b0799be1b4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019d3f3281754739820f990336bec1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/140 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f485619ae88e4bbe87778977d076d6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b53cbfa3ec5459ca83538d76a3e8285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab46771176bc4a4a9bf2f7bfbc2bbf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37d3d45401a4332bc490d03127371eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54aa6fff17146ce82f1b3cd11b2e770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True \n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:18.649033Z",
     "iopub.status.busy": "2025-02-06T15:17:18.648700Z",
     "iopub.status.idle": "2025-02-06T15:17:24.846522Z",
     "shell.execute_reply": "2025-02-06T15:17:24.845523Z",
     "shell.execute_reply.started": "2025-02-06T15:17:18.649003Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:24.848430Z",
     "iopub.status.busy": "2025-02-06T15:17:24.848014Z",
     "iopub.status.idle": "2025-02-06T15:17:42.046172Z",
     "shell.execute_reply": "2025-02-06T15:17:42.044878Z",
     "shell.execute_reply.started": "2025-02-06T15:17:24.848387Z"
    },
    "id": "6Z45_ods08sq",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1dBLjJhIV8olnOLLHnS_j0wFaPc1CzuZP\n",
      "To: /kaggle/working/test_dataset.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 389k/389k [00:00<00:00, 92.8MB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NLF9FeN3r9NkiNHsR2b1pweczlDw37kx\n",
      "To: /kaggle/working/train_dataset.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.56M/1.56M [00:00<00:00, 123MB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1CJ6F9LUcusyEKxihsjj3TqL2ofMwT7J5\n",
      "To: /kaggle/working/val_dataset.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80.1k/80.1k [00:00<00:00, 59.4MB/s]\n",
      "                                                Code  \\\n",
      "0  @Nonnull\\n        public Modal build()\\n      ...   \n",
      "1  public static Snowflake of(final Instant times...   \n",
      "2                    boolean isMentionRepliedUser();   \n",
      "3  public UnknownChannel(final GatewayDiscordClie...   \n",
      "4  public int getTypeRaw()\\n    {\\n        return...   \n",
      "\n",
      "                                             JavaDoc  \\\n",
      "0  /**\\n         * Builds and returns the {@link ...   \n",
      "1  /**\\n     * Constructs a {@code Snowflake} uti...   \n",
      "2  /**\\n     * Whether this message would mention...   \n",
      "3  /**\\n     * Constructs an {@code UnknownChanne...   \n",
      "4  /**\\n     * The raw type value used to derive ...   \n",
      "\n",
      "                                   Package  \n",
      "0  net.dv8tion.jda.api.interactions.modals  \n",
      "1                    discord4j.common.util  \n",
      "2       net.dv8tion.jda.api.utils.messages  \n",
      "3     discord4j.core.object.entity.channel  \n",
      "4                net.dv8tion.jda.api.audit  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!gdown 1dBLjJhIV8olnOLLHnS_j0wFaPc1CzuZP\n",
    "!gdown 1NLF9FeN3r9NkiNHsR2b1pweczlDw37kx\n",
    "!gdown 1CJ6F9LUcusyEKxihsjj3TqL2ofMwT7J5\n",
    "\n",
    "# Load the Excel file\n",
    "train_file_path = \"/kaggle/working/train_dataset.csv\"\n",
    "test_file_path = \"/kaggle/working/test_dataset.csv\"\n",
    "val_file_path = \"/kaggle/working/val_dataset.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "val_df = pd.read_csv(val_file_path)\n",
    "\n",
    "# Display the first few rows of the train and test sets to ensure everything works fine\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:42.048239Z",
     "iopub.status.busy": "2025-02-06T15:17:42.047802Z",
     "iopub.status.idle": "2025-02-06T15:17:42.055516Z",
     "shell.execute_reply": "2025-02-06T15:17:42.054348Z",
     "shell.execute_reply.started": "2025-02-06T15:17:42.048204Z"
    },
    "id": "JyMFCcbe2Smp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm_few_shot_examples = \"\"\"\n",
    "Here are a few examples\n",
    "\n",
    "@Nonnull\n",
    "public Modal build() {\n",
    "    Checks.check(!components.isEmpty(), \"Cannot make a modal without components!\");\n",
    "    Checks.check(components.size() <= MAX_COMPONENTS, \"Cannot make a modal with more than 5 components!\");\n",
    "\n",
    "    return new ModalImpl(id, title, components);\n",
    "}\n",
    "\n",
    "net.dv8tion.jda.api.interactions.modals\n",
    "\n",
    "/**\n",
    " * Builds and returns the {@link Modal}.\n",
    " *\n",
    " * @throws IllegalArgumentException\n",
    " *         <ul>\n",
    " *             <li>If no components are added</li>\n",
    " *             <li>If more than {@value MAX_COMPONENTS} component layouts are added</li>\n",
    " *         </ul>\n",
    " *\n",
    " * @return A Modal\n",
    " */\n",
    "\n",
    "@Nonnull\n",
    "@CheckReturnValue\n",
    "PermissionOverrideAction upsertPermissionOverride(@Nonnull IPermissionHolder permissionHolder);\n",
    "\n",
    "net.dv8tion.jda.api.entities.channel.attribute\n",
    "\n",
    "/**\n",
    " * Creates a new override or updates an existing one.\n",
    " * <br>This is similar to calling {@link PermissionOverride#getManager()} if an override exists.\n",
    " *\n",
    " * @param  permissionHolder\n",
    " *         The Member/Role for the override.\n",
    " *\n",
    " * @throws net.dv8tion.jda.api.exceptions.InsufficientPermissionException\n",
    " *         If we don't have the permission to {@link net.dv8tion.jda.api.Permission#MANAGE_PERMISSIONS MANAGE_PERMISSIONS}.\n",
    " * @throws java.lang.IllegalArgumentException\n",
    " *         If the provided permission holder is null or not from this guild.\n",
    " *\n",
    " * @return {@link net.dv8tion.jda.api.requests.restaction.PermissionOverrideAction}\n",
    " *         <br>With the current settings of an existing override or a fresh override with no permissions set.\n",
    " *\n",
    " * @see    PermissionOverrideAction#clear(long)\n",
    " * @see    PermissionOverrideAction#grant(long)\n",
    " * @see    PermissionOverrideAction#deny(long)\n",
    " */\n",
    "\n",
    "public LegacyEmbedCreateSpec setFooter(String text, @Nullable String iconUrl) {\n",
    "    requestBuilder.footer(EmbedFooterData.builder()\n",
    "        .text(text)\n",
    "        .iconUrl(iconUrl == null ? Possible.absent() : Possible.of(iconUrl))\n",
    "        .build());\n",
    "    return this;\n",
    "}\n",
    "\n",
    "discord4j.core.spec.legacy\n",
    "\n",
    "/**\n",
    " * Sets the footer of the embed.\n",
    " *\n",
    " * @param text The footer text.\n",
    " * @param iconUrl An icon URL to display in the footer.\n",
    " * @return This spec.\n",
    " */\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:42.057216Z",
     "iopub.status.busy": "2025-02-06T15:17:42.056911Z",
     "iopub.status.idle": "2025-02-06T15:17:42.072886Z",
     "shell.execute_reply": "2025-02-06T15:17:42.071764Z",
     "shell.execute_reply.started": "2025-02-06T15:17:42.057187Z"
    },
    "id": "28ktBT5_VVnI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm_one_shot_example = \"\"\"\n",
    "Here are an example\n",
    "\n",
    "@Nonnull\n",
    "public Modal build() {\n",
    "    Checks.check(!components.isEmpty(), \"Cannot make a modal without components!\");\n",
    "    Checks.check(components.size() <= MAX_COMPONENTS, \"Cannot make a modal with more than 5 components!\");\n",
    "\n",
    "    return new ModalImpl(id, title, components);\n",
    "}\n",
    "\n",
    "net.dv8tion.jda.api.interactions.modals\n",
    "\n",
    "/**\n",
    " * Builds and returns the {@link Modal}.\n",
    " *\n",
    " * @throws IllegalArgumentException\n",
    " *         <ul>\n",
    " *             <li>If no components are added</li>\n",
    " *             <li>If more than {@value MAX_COMPONENTS} component layouts are added</li>\n",
    " *         </ul>\n",
    " *\n",
    " * @return A Modal\n",
    " */\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:42.075361Z",
     "iopub.status.busy": "2025-02-06T15:17:42.074921Z",
     "iopub.status.idle": "2025-02-06T15:17:42.348860Z",
     "shell.execute_reply": "2025-02-06T15:17:42.347861Z",
     "shell.execute_reply.started": "2025-02-06T15:17:42.075315Z"
    },
    "id": "LjY75GoYUCB8",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3af52bb12948cfbe2791cab391fa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef57ec8ca4d48e4b183dfe69351da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6d29e02f5647f48262d8be9cb5a1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a5173b6ec945c69d1db09cf08fdab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a990457d81c4ac09043505463fbb298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_prompt = \"\"\"Given a function as Code and package of that function as Package, the task is to generate javadoc for the function.\n",
    "\n",
    "### Code:\n",
    "{}\n",
    "\n",
    "### Package:\n",
    "{}\n",
    "\n",
    "### JavaDoc:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "EOS_TOKEN = \"</eos>\" # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    codes = examples[\"Code\"]\n",
    "    packages = examples[\"Package\"]\n",
    "    javadocs = examples[\"JavaDoc\"]\n",
    "    texts = []\n",
    "    for code, package, javadoc in zip(codes, packages, javadocs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = llm_prompt.format(code, package, javadoc) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "def formatting_few_shot_prompts_func(examples):\n",
    "    codes = examples[\"Code\"]\n",
    "    packages = examples[\"Package\"]\n",
    "    javadocs = examples[\"JavaDoc\"]\n",
    "    texts = []\n",
    "    for code, package, javadoc in zip(codes, packages, javadocs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = llm_prompt.format(code, package, javadoc) + \"\\n\" +llm_few_shot_examples + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "def formatting_one_shot_prompts_func(examples):\n",
    "    codes = examples[\"Code\"]\n",
    "    packages = examples[\"Package\"]\n",
    "    javadocs = examples[\"JavaDoc\"]\n",
    "    texts = []\n",
    "    for code, package, javadoc in zip(codes, packages, javadocs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = llm_prompt.format(code, package, javadoc) + \"\\n\" +llm_one_shot_example + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched=True)\n",
    "test_few_shot = test_dataset.map(formatting_few_shot_prompts_func, batched=True)\n",
    "test_one_shot = test_dataset.map(formatting_one_shot_prompts_func, batched=True)\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:42.350681Z",
     "iopub.status.busy": "2025-02-06T15:17:42.350342Z",
     "iopub.status.idle": "2025-02-06T15:17:42.358158Z",
     "shell.execute_reply": "2025-02-06T15:17:42.357172Z",
     "shell.execute_reply.started": "2025-02-06T15:17:42.350652Z"
    },
    "id": "lQjGg5egPjb4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Code': 'public Integer getAnswerId() {\\n        return this.data.answerId().get(); // We can safely call get() here because the answerId is always present\\n    }',\n",
       " 'JavaDoc': '/**\\n     * Gets the answer id of this poll answer.\\n     *\\n     * @return the answer id of this poll answer\\n     */',\n",
       " 'Package': 'discord4j.core.object.entity.poll',\n",
       " 'text': 'Given a function as Code and package of that function as Package, the task is to generate javadoc for the function.\\n\\n### Code:\\npublic Integer getAnswerId() {\\n        return this.data.answerId().get(); // We can safely call get() here because the answerId is always present\\n    }\\n\\n### Package:\\ndiscord4j.core.object.entity.poll\\n\\n### JavaDoc:\\n/**\\n     * Gets the answer id of this poll answer.\\n     *\\n     * @return the answer id of this poll answer\\n     */</eos>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:42.361543Z",
     "iopub.status.busy": "2025-02-06T15:17:42.361211Z",
     "iopub.status.idle": "2025-02-06T15:17:43.948462Z",
     "shell.execute_reply": "2025-02-06T15:17:43.947591Z",
     "shell.execute_reply.started": "2025-02-06T15:17:42.361512Z"
    },
    "id": "6sBbH7UYFgbN",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe7ff945243466fa5790f9f145d41b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39962d0e027549f6b8b483a611828486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3549dd2937b84478a499e0c84338f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0123a83fd04c1d999d4ed581f4234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load BLEU and ROUGE metrics using the evaluate library\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids, labels\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions[0]\n",
    "\n",
    "    # Replace -100 with the pad token ID\n",
    "    pred_ids = np.where(pred_ids == -100, tokenizer.pad_token_id, pred_ids)\n",
    "    labels_ids = np.where(labels_ids == -100, tokenizer.pad_token_id, labels_ids)\n",
    "\n",
    "    # Decode the predicted and reference sequences\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_output = rouge_metric.compute(\n",
    "        predictions=pred_str,\n",
    "        references=label_str,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "    )\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_output = bleu_metric.compute(\n",
    "        predictions=pred_str,\n",
    "        references=[[ref] for ref in label_str]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": round(bleu_output[\"bleu\"], 4),\n",
    "        \"R1\": round(rouge_output[\"rouge1\"], 4),\n",
    "        \"R2\": round(rouge_output[\"rouge2\"], 4),\n",
    "        \"RL\": round(rouge_output[\"rougeL\"], 4),\n",
    "        \"RLsum\": round(rouge_output[\"rougeLsum\"], 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:43.950040Z",
     "iopub.status.busy": "2025-02-06T15:17:43.949657Z",
     "iopub.status.idle": "2025-02-06T15:17:50.997536Z",
     "shell.execute_reply": "2025-02-06T15:17:50.996658Z",
     "shell.execute_reply.started": "2025-02-06T15:17:43.949999Z"
    },
    "id": "kqF3EQtLlLG9",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6b57cfe20e8cd89d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6b57cfe20e8cd89d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/outputs/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T15:17:50.999785Z",
     "iopub.status.busy": "2025-02-06T15:17:50.999295Z",
     "iopub.status.idle": "2025-02-06T15:19:39.777300Z",
     "shell.execute_reply": "2025-02-06T15:19:39.775201Z",
     "shell.execute_reply.started": "2025-02-06T15:17:50.999738Z"
    },
    "id": "95_Nn-89DhsL",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e0a84ad1cf43cb9e6f55eb36aa94f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7140ac506884f448d3a186d5f1869ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/348 01:39 < 02:43, 1.32 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      7\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     ),\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# For zero shot results\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4073\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4070\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4072\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4073\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4074\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4083\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4287\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4284\u001b[0m         all_inputs\u001b[38;5;241m.\u001b[39madd(inputs_decode)\n\u001b[1;32m   4285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4286\u001b[0m     \u001b[38;5;66;03m# Pad labels here, preparing for preprocess_logits_for_metrics in next logits block.\u001b[39;00m\n\u001b[0;32m-> 4287\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4289\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2550\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:412\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:679\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    676\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:127\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    119\u001b[0m         {\n\u001b[1;32m    120\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m         }\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:659\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    660\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, \n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"tensorboard\",\n",
    "         # Evaluation settings\n",
    "        evaluation_strategy = \"steps\",  \n",
    "        eval_steps = 5,                \n",
    "        save_steps = 50,              \n",
    "        load_best_model_at_end = True,  \n",
    "    ),\n",
    ")\n",
    "# For zero shot results\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-06T15:19:39.780690Z",
     "iopub.status.idle": "2025-02-06T15:19:39.781224Z",
     "shell.execute_reply": "2025-02-06T15:19:39.780985Z",
     "shell.execute_reply.started": "2025-02-06T15:19:39.780958Z"
    },
    "id": "RqMLw83tWfI1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For one shot results\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_one_shot, #adding one shot\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 5, \n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"tensorboard\",\n",
    "         # Evaluation settings\n",
    "        evaluation_strategy = \"steps\",  \n",
    "        eval_steps = 5,                \n",
    "        save_steps = 50,               \n",
    "        load_best_model_at_end = True,  \n",
    "    ),\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-06T15:19:39.782913Z",
     "iopub.status.idle": "2025-02-06T15:19:39.783423Z",
     "shell.execute_reply": "2025-02-06T15:19:39.783190Z",
     "shell.execute_reply.started": "2025-02-06T15:19:39.783149Z"
    },
    "id": "8d_sJVf73MHU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For few shot results\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_few_shot,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 5, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"tensorboard\",\n",
    "         # Evaluation settings\n",
    "        evaluation_strategy = \"steps\",  \n",
    "        eval_steps = 5,                \n",
    "        save_steps = 50,               \n",
    "        load_best_model_at_end = True,  \n",
    "    ),\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-06T15:19:39.786048Z",
     "iopub.status.idle": "2025-02-06T15:19:39.786556Z",
     "shell.execute_reply": "2025-02-06T15:19:39.786327Z",
     "shell.execute_reply.started": "2025-02-06T15:19:39.786301Z"
    },
    "id": "53ShhVPp8DCb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset, #setting up the val dataset here\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 5, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"tensorboard\",\n",
    "         # Evaluation settings\n",
    "        evaluation_strategy = \"steps\",  \n",
    "        eval_steps = 20,                \n",
    "        save_steps = 60,               \n",
    "        load_best_model_at_end = True, \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-06T15:20:18.325Z",
     "iopub.execute_input": "2025-02-06T15:20:01.791205Z",
     "iopub.status.busy": "2025-02-06T15:20:01.790403Z"
    },
    "id": "yqxqAZ7KJ4oL",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,777 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 32 | Total steps = 87\n",
      " \"-____-\"     Number of trainable parameters = 29,884,416\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-06T15:19:39.790647Z",
     "iopub.status.idle": "2025-02-06T15:19:39.791160Z",
     "shell.execute_reply": "2025-02-06T15:19:39.790926Z",
     "shell.execute_reply.started": "2025-02-06T15:19:39.790900Z"
    },
    "id": "S8BnLvbJ8LvY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset, #changing to the test dataset\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 5, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"tensorboard\",\n",
    "         # Evaluation settings\n",
    "        evaluation_strategy = \"steps\", \n",
    "        eval_steps = 5,               \n",
    "        save_steps = 50,               \n",
    "        load_best_model_at_end = True,  \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-06T15:19:39.792537Z",
     "iopub.status.idle": "2025-02-06T15:19:39.793070Z",
     "shell.execute_reply": "2025-02-06T15:19:39.792841Z",
     "shell.execute_reply.started": "2025-02-06T15:19:39.792815Z"
    },
    "id": "B3VL_hIlCmof",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#For fine tuned results\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
